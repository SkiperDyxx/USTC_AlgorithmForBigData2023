# 降维

## 奇异值分解

### 用向量表示数据

数据集通常被表示成矩阵 $A\in\R^{n\times d}$，不妨记 $A = \begin{bmatrix}\vec a_1\\\vec a_2\\\vdots\\\vec a_n\end{bmatrix}$，每个行向量 $\vec a_i\in\R^d$ 都是一个数据

**奇异值分解**会找到一个 *最适合*（"best-fitting"）的 $k$ 维 **线性子空间**

*最适合* 这里指这些点到这个子空间距离的平方和最小，就跟最小二乘法一样。也就是说找到一个线性子空间 $S$，使得：
$$
S = \arg\min_{S \text{ is a } k-\text{dim subspace }}\sum_{i = 1}^n{\rm dist}^2(\vec a_i, S)
$$
> $\arg$ 表示主值，这里 $\arg\min_S f(S)$ 表示当 $f(S)$ 取到最小值时 $S$ 的值，有多个最小值时任取一个

考虑勾股定理，这里有 ${\rm dist}^2(\vec a_i, S) + {\rm length}^2({\rm proj}(\vec a_i, S)) = {\rm length}^2(\vec a_i)$，而 ${\rm length}(\vec a_i)$ 是固定值，所以 $S$ 的一个等价表述为：
$$
\begin{aligned}
S &= \arg\min_{S \text{ is a } k-\text{dim subspace }}\sum_{i = 1}^n{\rm dist}^2(\vec a_i, S) \\
&= \arg\min_{S \text{ is a } k-\text{dim subspace }}\left(\sum_{i = 1}^n{\rm length}^2(a_i) - \sum_{i = 1}^n{\rm length}^2({\rm proj}(\vec a_i, S))\right)\\
&= \arg\max_{S \text{ is a }k-\text{dim subspace}}\sum_{i = 1}^n{\rm length}^2({\rm proj}(\vec a_i, S))
\end{aligned}
$$

### 奇异向量与奇异值

回忆线性代数中的 **范数**。若 $p\geq 1$，则记 $p$ 范数为 $\norm{\vec a}_p = \left(\sum_{i = 1}^n\abs{\vec a_i}^p\right)^{\frac 1p}$。那么有：
$$
\norm{\vec a}_2 = \sqrt{\sum_{i = 1}^na_i^2}\\
\norm{\vec a}_1 = \sum_{i = 1}^n\abs{a_i}\\
\norm{\vec a}_\infty = \max_{i = 1}^n\abs{a_i}\\
$$
在本章节，范数默认为 $2$ 范数，即默认 $\norm{\vec a} = \norm{\vec a}_2$。

（额外地，定义 $\norm{\vec a}_0$ 是 $\vec a$ 中非零元的数目）

注意到前文奇异值分解要找到的是线性子空间，也就是说这个子空间一定是过原点的

当 $k = 1$ 时，任意的一维线性子空间都可以被表述成 $1$ 个非零向量 $\vec v\in\R^d$ 张成的空间，不妨假设 $\vec v$ 的长度 $\norm{\vec v}_2$ 为 $1$。要找到这个子空间，就等价于找到这个单位向量 $\vec v$

记两个列向量 $\vec u, \vec v\in\R^d$ 的内积为 $\vec u^\intercal \vec v$ 或 $\left\langle\vec u, \vec v\right\rangle$

在这里记 ${\rm proj}(\vec a_i, \vec v) = {\rm proj}(\vec a_i, S)$，那么有 ${\rm length}({\rm proj}(\vec a_i, \vec v)) = \left\langle \vec a_i^\intercal, \vec v\right\rangle$（想想内积的几何性质），所以：
$$
\sum_{i = 1}^n{\rm length}^2({\rm proj}(\vec a_i, S)) = \sum_{i = 1}^n\left\langle \vec a_i^\intercal, \vec v\right\rangle^2 = \sum_{i = 1}^n\left(\sum_{j = 1}^na_{i, j}v_j\right)^2 = \norm{A\vec v}_2^2
$$
记 $A$ 的第一个右奇异向量 $\vec v_1 = \arg\max_{\norm{\vec v}_1 = 1}\norm{A\vec v}_2$，同时记 $A$ 的第一个奇异值 $\sigma_1(A) = \norm{A\vec v_1}_2$，那么 $\vec v_1$ 张成的线性子空间即为所求，而且所有数据向量在 $\vec v_1$ 张成的线性子空间上投影平方和为 $\sigma_1^2(A)$。

当 $k = 2$ 时，一个 *贪心* 的做法是在 $\vec v_1$ 张成的子空间基础上再新增一个维度，也就是再找一个向量 $\vec v_2\perp \vec v_1$，然后取 $S = {\rm span}(\vec v_1, \vec v_2)$。所以我们先定义 $A$ 的第二个右奇异向量与第二个奇异值：
$$
\vec v_2 = \arg\max_{\norm{\vec v}_2 = 1, \vec v\perp \vec v_1}\norm{A\vec v}_2\\
\sigma_2(A) = \norm{A\vec v_2}_2
$$
后续再证明这样贪心定义的右奇异向量确实能张成前文中的最优子空间

类似地，可以定义第 $i$ 个右奇异向量与第 $i$ 个奇异值：
$$
\vec v_i = \arg\max_{\norm{\vec v}_2 = 1, \vec v_i \perp \vec v_1, \cdots, \vec v_i\perp \vec v_{i - 1}}\norm{A\vec v}_2\\
\sigma_i(A) = \norm{A\vec v_i}_2
$$
假设已有在任意给定 $\R^d$ 的子空间 $T$ 内求解 $\arg\max_{\norm{\vec v}_2 = 1, \vec v\in T}\norm{A\vec v}_2$ 的（最优化）算法，那么可以很容易地设计一个求解所有右奇异向量的算法

接下来证明以上贪心做法的最优性

**定理 1：**设 $A\in\R^{n\times d}$，贪心定义的右奇异向量为 $\vec v_1, \vec v_2, \cdots, \vec v_r$。对于任意的 $1\leq k\leq r$，取 $V_k = {\rm span}(\vec v_1, \cdots, \vec v_k)$，则 $V_k$ 确实是 *最适合* $k$ 维线性子空间，其中 $r$ 是矩阵 $A$ 的 **秩**

> 为什么这里会提到秩呢？若 $A$ 不列满秩，也就是当 $r < d$ 时，尝试构造 $\vec v_{r + 1}$ 的时候会发生什么呢？根据行空间、零空间与秩的性质可以得知 $V_r$ 以外的所有向量都在 $A$ 的零空间内，也就是不论怎么贪心地选取 $\vec v_{r + 1}$，都有 $A\vec v_{r + 1} = \vec 0$

在证明这个定理之前，先说明一个子空间内投影的性质：

若 $\vec u_1, \vec u_2, \cdots, \vec u_k$ 是 $\R^d$ 的 $k$ 维线性子空间 $S$ 的一组单位正交基，则
$$
\sum_{i = 1}^n{\rm length}^2({\rm proj}(\vec a_i, S)) = \sum_{i = 1}^n\sum_{j = 1}^k\left\langle \vec a_i^\intercal, \vec v_k\right\rangle^2 = \sum_{j =1}^k\norm{A\vec v_j}_2^2
$$
这两个等号成立的理由与计算 $\vec v_1$ 时的推导类似。

**证明：**

采用数学归纳法

$k = 1$ 的时候，联想 $\vec v_1$ 的定义我们没有贪心，自然 $V_1$ 是线性子空间

下尝试论证 $V_{k - 1}$ 是 *最适合* 线性子空间可以推得 $V_k$ 是 *最适合* 线性子空间。设某个 *最适合* $k$ 维线性子空间是 $W$，接下来尝试证 $V_k = W$

因为 $W$ 是 $k$ 维线性空间，所以 $W$ 存在一组大小为 $k$ 的单位正交基 $\vec w_1, \vec w_2, \cdots, \vec w_k$ 且满足 $\vec w_k\perp \vec v_1, \vec v_2, \cdots, \vec v_{k - 1}$，其中 $\vec v_1, \vec v_2, \cdots, \vec v_{k - 1}$ 是贪心算法算出的前 $k - 1$ 个右奇异向量

由归纳假设，因为 $V_{k - 1}$ 是 *最适合* 线性子空间，也就是说 $A$ 在 $V_{k - 1}$ 上的投影不会比 $A$ 在 $\qty{\vec w_1, \vec w_2, \cdots, \vec w_{k - 1}}$ 张成的子空间上的投影更短，联想前文所述投影的性质，有：
$$
\begin{aligned}
&\norm{A\vec w_1}^2 + \norm{A\vec w_2}^2 + \cdots + \norm{A\vec w_{k - 1}}^2\\
\leq &\norm{A\vec v_1}^2 + \norm{A\vec v_2}^2 + \cdots + \norm{A\vec v_{k - 1}}^2
\end{aligned}
$$
联想为贪心算法构造的第 $k$ 个右奇异向量 $\vec v_k$ 的定义：
$$
\vec v_i = \arg\max_{\norm{\vec v}_2 = 1, \vec v_i \perp \vec v_1, \cdots, \vec v_i\perp \vec v_{i - 1}}\norm{A\vec v}_2
$$
$\vec v_k$ 满足 $\norm{A\vec v_k}$ 是所有垂直于 $\vec v_1, \vec v_2, \cdots, \vec v_{k - 1}$ 的单位向量 $\vec v$ 中 $\norm{A\vec v}$ 最大的一个，而且 $\vec w_k$ 也是一个垂直于 $\vec v_1, \vec v_2, \cdots, \vec v_{k - 1}$ 的向量，所以
$$
\norm{A\vec w_k}_2^2\leq \norm{A\vec v_k}_2^2
$$
两个不等式相加知：
$$
\begin{aligned}
&\norm{A\vec w_1}^2 + \norm{A\vec w_2}^2 + \cdots + \norm{A\vec w_k}^2\\
\leq &\norm{A\vec v_1}^2 + \norm{A\vec v_2}^2 + \cdots + \norm{A\vec v_k}^2
\end{aligned}
$$
联想投影的性质，上式说明 $k$ 维子空间 $V_k$ 不会比 $W$ 更 *不适合*，而 $W$ 已经是 *最适合* 的 $k$ 维子空间了，所以 $V_k$ 也是一个 *最适合* 的 $k$ 维子空间

### 奇异值与 Frobenious 范数

首先定义矩阵 $A\in\R^{n\times m}$ 的 **Frobenious范数** 为矩阵中所有元素的平方和：
$$
\norm{A}_F = \sqrt{\sum_{i = 1}^n\sum_{j = 1}^ma_{i, j}^2}
$$
那么我们不难证明

**定理：**矩阵 Frobenious 范数的平方等于其全体奇异值的平方和
$$
\norm{A}_F^2 = \sum_{i = 1}^r\sigma_i^2(A)
$$

证明：

一方面，由定义（或者说由勾股定理）
$$
\sum_{j = 1}^n\norm{\vec a_j}^2 = \sum_{j = 1}^n\sum_{k = 1}^da_{jk}^2 = ||A||_F^2
$$
另一方面：
$$
\sum_{j = 1}^n\norm{\vec a_j}^2 = \sum_{j = 1}^n\sum_{i = 1}^r\left\langle\vec a_j,\vec v_i\right\rangle^2 = \sum_{i = 1}^r\norm{A\vec v_i}^2
$$

> 上式第一个等号成立是因为由奇异值分解的过程知 ${\rm span}\qty{\vec v_1, \vec v_2, \cdots, \vec v_r}$ 以外的向量都在 $A$ 的零空间内

### 奇异值分解

设 $A\in\R^{n\times d}$ 有右奇异向量 $\vec v_1, \vec v_2, \cdots, \vec v_r$ 与对应的奇异值 $\sigma_1(A), \sigma_2(A), \cdots, \sigma_r(A)$。定义 $A$ 的 $r$ 个左奇异向量：
$$
\vec u_i = \frac{A\vec v_i}{\sigma_i(A)}
$$
因为奇异值的定义是 $\sigma_i(A) = \norm{A\vec v_i}_2$，所以不难看出左奇异向量也是单位向量

**引理：**左奇异向量两两正交

**证明：**反证

假设存在某些左奇异向量两两不正交，那么我们取最小的 $i$ 满足 $\exists j > i\text { s.t. } u_i\cancel\perp u_j$

**不失一般性**，假设 $\left\langle \vec u_i, \vec u_j\right\rangle > 0$，记 $\delta\triangleq \left\langle\vec u_i, \vec u_j\right\rangle$

> 为什么这个假设“不失一般性”呢？读者若不理解，可以将这视为分类讨论，然后尝试用几乎完全相同的方法证明 $\left\langle \vec u_i, \vec u_j\right\rangle < 0$ 的场景

对于 $\epsilon > 0$，设
$$
\vec v'(\epsilon) = \frac{\vec v_i + \epsilon \vec v_j}{\norm{\vec v_i + \epsilon \vec v_j}_2}
$$
>  简记 $\vec v'(\epsilon)$ 为 $\vec v'$，但读者请记住 $\vec v'$ 随 $\epsilon$ 变化而变化

则有：

$$
\norm{\vec v_i + \epsilon\vec v_j} = \sqrt{\left\langle\vec v_i + \epsilon \vec v_j, \vec v_i + \epsilon\vec v_j\right\rangle} = \sqrt{1 + \epsilon^2}\\
A\vec v' = \frac{A\vec v_i + \epsilon A\vec v_j}{\norm{\vec v_i + \epsilon \vec v_j}_2} = \frac{\sigma_i\vec u_i + \epsilon\sigma_j \vec u_j}{\sqrt{1 + \epsilon^2}}
$$
注意到 $\vec u_i$ 是单位向量，而且由柯西不等式有：
$$
\norm{A\vec v'}_2 = \norm{\vec u_i}_2\norm{A\vec v'}_2 \geq \vec u_i^\intercal A\vec v'
$$
再代入上面对 $A\vec v'$ 的分析与反证假设 $\vec u_i^\intercal\vec u_j = \delta > 0$ 有：
$$
\norm{A\vec v'}_2\geq \vec u_i^\intercal \frac{\sigma_iu_i + \epsilon\sigma_j\vec u_j}{\sqrt{1 + \epsilon^2}} = \frac{\sigma_i + \epsilon\delta\sigma_j}{\sqrt{1 + \epsilon^2}}
$$
回忆定义时就规定 $\epsilon > 0$，代入不等式 $(1 - \epsilon^2 / 2)^2(1 + \epsilon^2)< 1$

> 这个不等式可以由 $(1 + \epsilon^2)^{-\frac 12}$ 在 $\epsilon = 0$ 处泰勒展开得到

$$
\norm{A\vec v'}_2\geq \frac{\sigma_i + \epsilon\delta\sigma_j}{\sqrt{1 + \epsilon^2}} > (\sigma_i + \epsilon\delta\sigma_j)\qty(1 - \frac{\epsilon^2}2) = \sigma_i +  \epsilon\qty(\sigma_j\delta - \frac{\sigma_i}2\epsilon - \frac{\sigma_j\delta}2\epsilon^2)
$$
只要 $\epsilon$ 充分小，$\sigma_j\delta - \frac{\sigma_i}2\epsilon - \frac{\sigma_j\delta}2\epsilon^2$ 总能变成一个正数，这表明 $\epsilon$ 充分小时 $\norm{A\vec v'}_2 > \sigma_i$ 成立，取定一个这样充分小的 $\epsilon$ 与其对应的 $\vec v'$

> 上面的文字也可以可以简便地写作：
>
> 求导
> $$
> \dv{\norm{A\vec v'}_2}{\epsilon} > 0
> $$
> 只要 $\epsilon$ 足够小就有 $\norm{A\vec v'(\epsilon)}_2 > \norm{A\vec v'(0)}_2 = \norm{A\vec v_i}$，取定一个这样的 $\epsilon$ 与 $\vec v'$。

另一方面，$\vec v'$ 是 $\vec v_i$ 与 $\vec v_j$ 的线性组合，而 $\vec v_i, \vec v_j$ 均与 $\vec v_1, \vec v_2, \cdots, \vec v_{i - 1}$ 垂直，这就表明 $\vec v'$ 也与 $\vec v_1, \vec v_2, \cdots, \vec v_{i - 1}$ 垂直。而 $\norm{A\vec v'}_2 > \norm{A\vec v_i}_2$，这与 $\vec v_i$ 的定义矛盾

>  $\vec v_i$ 是所有与 $\vec v_1, \vec v_2, \cdots, \vec v_{i - 1}$ 垂直的单位向量 $v$ 中 $\norm{A\vec v}_2$ 最大的一个，然而 $\vec v'$ 不但满足单位向量条件与垂直条件，$\norm{A\vec v'}_2$ 还比 $\norm{A\vec v_i}_2$ 更大

故假设不成立

<hr>

有了以上的准备工作，接下来我们便可以证明

**定理：**
$$
A = \sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal
$$

**证明：**

首先注意到一个事实：若 $B, C\in\R^{n\times d}$，则：
$$
B = C\iff B\vec v = C\vec v,\forall v\in\R^d
$$
为了证明定理成立，只要证明：对于任意的 $\vec v\in\R^d$，都有
$$
A\vec v = \qty(\sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal)\vec v
$$
下证明上式。首先注意到对于一类特殊的向量——$A$ 的 $r$ 个右奇异向量均有：
$$
A\vec v_i = \sigma_i\vec u_i = \qty(\sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal)\vec v_i
$$
>  第二个等号成立是因为 $A$ 的右奇异值两两正交 $j\neq i\implies \vec v_j^\intercal \vec v_i = 0$

但是考虑到 $r\leq d$，也就是不一定有 $r = d$。所以我们先补齐右奇异向量，任意地将 $\vec v_1, \vec v_2, \cdots, \vec v_r$ 补齐成 $\R^d$ 的一组单位正交基 $\vec v_1, \vec v_2, \cdots, \vec v_d$，也就是选择单位向量 $\vec v_{r + 1}, \cdots, \vec v_d$ 满足 $\vec v_1, \vec v_2, \cdots, \vec v_d$ 两两正交

补齐了之后就可以在这组基上展开 $\vec v$：设 $\left\langle\vec v, \vec v_i\right\rangle = \alpha_i$，则
$$
\vec v = \sum_{i = 1}^d\alpha_i\vec v_i
$$
因为 $A$ 的秩为 $r$，而 $\vec v_1, \vec v_2, \cdots, \vec v_r$ 都不在 $A$ 的（右）零空间中，所以 $A\vec v_{r + 1} = A\vec v_{r + 2} = \cdots = A\vec v_d = \vec 0$。所以
$$
A\vec v = A\sum_{i = 1}^d\alpha_i\vec v_i = \sum_{i = 1}^d\alpha_iA\vec v_i = \sum_{i = 1}^r\alpha_i\sigma_i\vec u_i\\
\qty(\sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal)\vec v = \qty(\sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal)\qty(\sum_{i = 1}^d\alpha_i\vec v_i) = \sum_{i = 1}^r\sum_{j = 1}^d\sigma_i\alpha_j\vec u_i\vec v_i^\intercal\vec v_j = \sum_{i = 1}^r\sigma_i\alpha_i\vec u_i = A\vec v
$$
对于任意的 $\vec v\in\R^d$ 上式均成立，由前文提及的事实，定理证毕

记
$$
U\triangleq \qty[\vec u_1, \vec u_2, \cdots, \vec u_r]\in\R^{n\times r}, \Sigma \triangleq{\rm diag}\qty(\sigma_1, \sigma_2, \cdots, \sigma_r)\in\R^{r\times r}, V\triangleq\qty[\vec v_1, \vec v_2, \cdots, \vec v_r]\in \R^{d\times r}
$$
由上定理知
$$
A = U\Sigma V^\intercal
$$
这称为矩阵 $A$ 的奇异值分解
