# 降维

## 奇异值分解

### 用向量表示数据

数据集通常被表示成矩阵 $A\in\R^{n\times d}$，不妨记 $A = \begin{bmatrix}\vec a_1\\\vec a_2\\\vdots\\\vec a_n\end{bmatrix}$，每个行向量 $\vec a_i\in\R^d$ 都是一个数据

**奇异值分解**会找到一个 *最适*（"best-fitting"）的 $k$ 维 **线性子空间**

*最适* 这里指这些点到这个子空间距离的平方和最小，就跟最小二乘法一样。也就是说找到一个线性子空间 $S$，使得（$\arg$ 表示主值，这里 $\arg\min_S f(S)$ 表示当 $f(S)$ 取到最小值时 $S$ 的值）：
$$
S = \arg\min_{S \text{ is a } k-\text{dim subspace }}\sum_{i = 1}^n{\rm dist}^2(\vec a_i, S)
$$
考虑勾股定理，这里有 ${\rm dist}^2(\vec a_i, S) + {\rm length}^2({\rm proj}(\vec a_i, S)) = {\rm length}^2(\vec a_i)$，而 ${\rm length}(\vec a_i)$ 是固定值，所以 $S$ 的一个等价表述为：
$$
\begin{aligned}
S &= \arg\min_{S \text{ is a } k-\text{dim subspace }}\sum_{i = 1}^n{\rm dist}^2(\vec a_i, S) \\
&= \arg\min_{S \text{ is a } k-\text{dim subspace }}\left(\sum_{i = 1}^n{\rm length}^2(a_i) - \sum_{i = 1}^n{\rm length}^2({\rm proj}(\vec a_i, S))\right)\\
&= \arg\max_{S \text{ is a }k-\text{dim subspace}}\sum_{i = 1}^n{\rm length}^2({\rm proj}(\vec a_i, S))
\end{aligned}
$$

### 奇异向量与奇异值

回忆线性代数中的 **范数**。若 $p\geq 1$，则记 $p$ 范数为 $\norm{\vec a}_p = \left(\sum_{i = 1}^n\abs{\vec a_i}^p\right)^{\frac 1p}$。那么有：
$$
\norm{\vec a}_2 = \sqrt{\sum_{i = 1}^na_i^2}\\
\norm{\vec a}_1 = \sum_{i = 1}^n\abs{a_i}\\
\norm{\vec a}_\infty = \max_{i = 1}^n\abs{a_i}\\
$$
在本章节，范数默认为 $2$ 范数，即默认 $\norm{\vec a} = \norm{\vec a}_2$。

（额外地，定义 $\norm{\vec a}_0$ 是 $\vec a$ 中非零元的数目）

注意到前文奇异值分解要找到的是线性子空间，也就是说这个子空间一定是过原点的

当 $k = 1$ 时，任意的一维线性子空间都可以被表述成 $1$ 个非零向量 $\vec v\in\R^d$ 张成的空间，不妨假设 $\vec v$ 的长度 $\norm{\vec v}_2$ 为 $1$。要找到这个子空间，就等价于找到这个单位向量 $\vec v$

记两个列向量 $\vec u, \vec v\in\R^d$ 的内积为 $\vec u^\intercal \vec v$ 或 $\left\langle\vec u, \vec v\right\rangle$

在这里记 ${\rm proj}(\vec a_i, \vec v) = {\rm proj}(\vec a_i, S)$，那么有 ${\rm length}({\rm proj}(\vec a_i, \vec v)) = \left\langle \vec a_i^\intercal, \vec v\right\rangle$（想想内积的几何性质），所以：
$$
\sum_{i = 1}^n{\rm length}^2({\rm proj}(\vec a_i, S)) = \sum_{i = 1}^n\left\langle \vec a_i^\intercal, \vec v\right\rangle^2 = \sum_{i = 1}^n\left(\sum_{j = 1}^na_{i, j}v_j\right)^2 = \norm{A\vec v}_2^2
$$
记 $A$ 的第一个奇异向量 $\vec v_1 = \arg\max_{\norm{\vec v}_1 = 1}\norm{A\vec v}_2$，同时记 $A$ 的第一个奇异值 $\sigma_1(A) = \norm{A\vec v_1}_2$，那么 $\vec v_1$ 张成的线性子空间即为所求，而且所有数据向量在 $\vec v_1$ 张成的线性子空间上投影平方和为 $\sigma_1^2(A)$。

当 $k = 2$ 时，一个 *贪心* 的做法是在 $\vec v_1$ 张成的子空间基础上再新增一个维度，也就是再找一个向量 $\vec v_2\perp \vec v_1$，然后取 $S = {\rm span}(\vec v_1, \vec v_2)$。所以我们先定义 $A$ 的第二个奇异向量与第二个奇异值：
$$
\vec v_2 = \arg\max_{\norm{\vec v}_2 = 1, \vec v\perp \vec v_1}\norm{A\vec v}_2\\
\sigma_2(A) = \norm{A\vec v_2}_2
$$
后续再证明这样贪心定义的奇异向量确实能张成前文中的最优子空间

类似地，可以定义第 $i$ 个奇异向量与第 $i$ 个奇异值：
$$
v_i = \arg\max_{\norm{\vec v}_2 = 1, \vec v_i \perp \vec v_1, \cdots, \vec v_i\perp \vec v_{i - 1}}\norm{A\vec v}_2\\
\sigma_i(A) = \norm{A\vec v_i}_2
$$
假设已有在任意给定 $\R^d$ 的子空间 $T$ 内求解 $\arg\max_{\norm{\vec v}_2 = 1, \vec v\in T}\norm{A\vec v}_2$ 的（最优化）算法，那么可以很容易地设计一个求解所有奇异向量的算法

接下来证明以上贪心做法的最优性

**定理 1：**设 $A\in\R^{n\times d}$，贪心定义的奇异向量为 $\vec v_1, \vec v_2, \cdots, \vec v_r$。对于任意的 $1\leq k\leq r$，取 $V_k = {\rm span}(\vec v_1, \cdots, \vec v_k)$，则 $V_k$ 确实是 *最适* $k$ 维线性子空间，其中 $r$ 是矩阵 $A$ 的 **秩**

**证明：**

采用数学归纳法

$k = 1$ 的时候，联想 $\vec v_1$ 的定义我们没有贪心，自然 $V_1$ 是线性子空间

假设 $k - 1$ 的时候成立。（未完待续）